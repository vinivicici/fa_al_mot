{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c838c1",
   "metadata": {},
   "source": [
    "# CLIP 기반 이미지/텍스트 임베딩 파이프라인 (정리본)\n",
    "\n",
    "이 노트북은 다음을 수행합니다.\n",
    "\n",
    "1. CSV 로드 및 전처리 (`description`, `dataset_source`, `product_id`)\n",
    "2. `dataset_source`별 이미지 경로 매칭 (HNM / FASHION)\n",
    "3. CLIP(`openai/clip-vit-base-patch32`)로 텍스트/이미지 임베딩 추출 및 L2 정규화\n",
    "4. 임베딩을 CSV로 저장 (`img_emb`, `wrd_emb`)\n",
    "\n",
    "> 제출용을 가정하여 **경로/하이퍼파라미터를 상단 Config로 통일**하고, 함수 단위로 정리했습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36548ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (선택) 최초 1회 설치 - 이미 환경에 설치돼 있으면 생략 가능\n",
    "# !pip install -q transformers torch torchvision pillow tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea2f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9734df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Config (여기만 수정)\n",
    "# =========================\n",
    "CSV_PATH    = Path(r\"C:\\Users\\min\\Downloads\\csvs\\merged_dataset_onehot.csv\")\n",
    "\n",
    "HNM_DIR     = Path(r\"C:\\Users\\min\\Downloads\\filtered_images\")                      # HNM images\n",
    "FASHION_DIR = Path(r\"C:\\Users\\min\\Downloads\\archive (13)\\fashion-dataset\\images\")  # FASHION images\n",
    "\n",
    "OUT_CSV     = Path(r\"C:\\Users\\min\\Downloads\\before_parquet.csv\")\n",
    "\n",
    "MODEL_NAME  = \"openai/clip-vit-base-patch32\"\n",
    "BATCH_SIZE  = 64\n",
    "NUM_WORKERS = 0   # Windows/Jupyter 환경에서는 0 권장\n",
    "\n",
    "# HNM 파일명 규칙: '0108775015.jpg' -> code=fname[1:7] == '108775'\n",
    "HNM_CODE_SLICE = slice(1, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Model / Device\n",
    "# =========================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(MODEL_NAME).to(device).eval()\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def l2_normalize(x: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    return x / (x.norm(dim=-1, keepdim=True) + eps)\n",
    "\n",
    "print(f\"Device={device} | Model={MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb54c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# I/O & Validation Utils\n",
    "# =========================\n",
    "REQUIRED_COLS = {\"description\", \"dataset_source\", \"product_id\"}\n",
    "VALID_SOURCES = {\"HNM\", \"FASHION\"}\n",
    "\n",
    "def load_and_filter_csv(csv_path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    missing = REQUIRED_COLS - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV에 필요한 컬럼이 없습니다: {sorted(missing)}\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"description\"] = df[\"description\"].fillna(\"\").astype(str)\n",
    "    df = df[df[\"description\"].str.len() > 0]\n",
    "\n",
    "    df[\"dataset_source\"] = df[\"dataset_source\"].astype(str).str.upper().str.strip()\n",
    "    df = df[df[\"dataset_source\"].isin(VALID_SOURCES)]\n",
    "\n",
    "    # product_id 정규화: 공백 제거, 숫자로 읽힌 '... .0' 제거\n",
    "    def norm_pid(x) -> str:\n",
    "        s = \"\" if pd.isna(x) else str(x).strip()\n",
    "        return s[:-2] if s.endswith(\".0\") else s\n",
    "\n",
    "    df[\"product_id\"] = df[\"product_id\"].map(norm_pid)\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "df = load_and_filter_csv(CSV_PATH)\n",
    "print(\"전처리 후 행 수:\", len(df))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04112263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Image Index (HNM / FASHION)\n",
    "# =========================\n",
    "def build_hnm_index(root_dir: Path) -> Dict[str, str]:\n",
    "    \"\"\"HNM: 파일명에서 code=fname[1:7] 추출 -> code -> 이미지 경로 (첫 번째로 발견된 파일)\"\"\"\n",
    "    code2path: Dict[str, str] = {}\n",
    "    for p in root_dir.rglob(\"*.jpg\"):\n",
    "        fname = p.name\n",
    "        if len(fname) >= 7:\n",
    "            code = fname[HNM_CODE_SLICE]\n",
    "            code2path.setdefault(code, str(p))\n",
    "    return code2path\n",
    "\n",
    "def build_fashion_index(root_dir: Path) -> Dict[str, str]:\n",
    "    \"\"\"FASHION: 파일명(확장자 제외) == product_id\"\"\"\n",
    "    name2path: Dict[str, str] = {}\n",
    "    for p in root_dir.rglob(\"*.jpg\"):\n",
    "        name2path[p.stem] = str(p)\n",
    "    return name2path\n",
    "\n",
    "print(\"HNM 인덱스 생성 중...\")\n",
    "hnm_idx = build_hnm_index(HNM_DIR)\n",
    "print(\"HNM 인덱스 크기:\", len(hnm_idx))\n",
    "\n",
    "print(\"FASHION 인덱스 생성 중...\")\n",
    "fashion_idx = build_fashion_index(FASHION_DIR)\n",
    "print(\"FASHION 인덱스 크기:\", len(fashion_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bda3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Row-wise image path resolver\n",
    "# =========================\n",
    "def resolve_image_path(dataset_source: str, product_id: str) -> Optional[str]:\n",
    "    if dataset_source == \"HNM\":\n",
    "        # 1) code direct match (product_id == 6-digit code)\n",
    "        if product_id in hnm_idx:\n",
    "            return hnm_idx[product_id]\n",
    "\n",
    "        # 2) fallback: product_id가 파일명에 포함되는지 느슨하게 검색 (매우 느릴 수 있음)\n",
    "        USE_FALLBACK_SCAN = False\n",
    "        if USE_FALLBACK_SCAN:\n",
    "            for p in HNM_DIR.rglob(\"*.jpg\"):\n",
    "                if product_id in p.name:\n",
    "                    return str(p)\n",
    "        return None\n",
    "\n",
    "    if dataset_source == \"FASHION\":\n",
    "        return fashion_idx.get(product_id)\n",
    "\n",
    "    return None\n",
    "\n",
    "df = df.copy()\n",
    "df[\"image_path\"] = [\n",
    "    resolve_image_path(src, pid)\n",
    "    for src, pid in tqdm(zip(df[\"dataset_source\"].tolist(), df[\"product_id\"].tolist()), total=len(df))\n",
    "]\n",
    "\n",
    "before = len(df)\n",
    "df = df[df[\"image_path\"].notna()].reset_index(drop=True)\n",
    "after = len(df)\n",
    "print(f\"이미지 매칭 성공: {after}/{before} ({after/before:.1%})\")\n",
    "\n",
    "df.head(3)[[\"dataset_source\", \"product_id\", \"image_path\", \"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a164151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CLIP Encoding\n",
    "# =========================\n",
    "def encode_texts_clip(texts: List[str], batch_size: int = BATCH_SIZE) -> torch.Tensor:\n",
    "    outs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Text batches\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = processor(text=batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            feats = model.get_text_features(**inputs)\n",
    "            feats = l2_normalize(feats)\n",
    "        outs.append(feats.cpu())\n",
    "        del inputs, feats\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    return torch.cat(outs, dim=0)\n",
    "\n",
    "def encode_images_clip(paths: List[str], batch_size: int = BATCH_SIZE) -> torch.Tensor:\n",
    "    outs = []\n",
    "    for i in tqdm(range(0, len(paths), batch_size), desc=\"Image batches\"):\n",
    "        batch_paths = paths[i:i+batch_size]\n",
    "        images = [Image.open(p).convert(\"RGB\") for p in batch_paths]\n",
    "\n",
    "        inputs = processor(images=images, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            feats = model.get_image_features(**inputs)\n",
    "            feats = l2_normalize(feats)\n",
    "        outs.append(feats.cpu())\n",
    "\n",
    "        # cleanup\n",
    "        for img in images:\n",
    "            try:\n",
    "                img.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "        del images, inputs, feats\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    return torch.cat(outs, dim=0)\n",
    "\n",
    "texts = df[\"description\"].tolist()\n",
    "img_paths = df[\"image_path\"].tolist()\n",
    "\n",
    "wrd_emb_t = encode_texts_clip(texts)\n",
    "img_emb_t = encode_images_clip(img_paths)\n",
    "\n",
    "wrd_emb = wrd_emb_t.numpy()\n",
    "img_emb = img_emb_t.numpy()\n",
    "\n",
    "print(\"wrd_emb:\", wrd_emb.shape, \"img_emb:\", img_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce1b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (선택) 간단 검증: 같은 행의 텍스트-이미지 코사인 유사도\n",
    "pair_cos = np.sum(wrd_emb * img_emb, axis=1)\n",
    "print(\"pair cosine mean:\", float(pair_cos.mean()))\n",
    "print(\"pair cosine median:\", float(np.median(pair_cos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Save to CSV\n",
    "# =========================\n",
    "def vec_to_str(v: np.ndarray) -> str:\n",
    "    return \",\".join(f\"{x:.6f}\" for x in v.tolist())\n",
    "\n",
    "df_out = df.copy()\n",
    "df_out[\"img_emb\"] = [vec_to_str(v) for v in img_emb]\n",
    "df_out[\"wrd_emb\"] = [vec_to_str(v) for v in wrd_emb]\n",
    "\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"CSV 저장 완료: {OUT_CSV}\")\n",
    "print(\"열:\", list(df_out.columns))\n",
    "df_out.head(2)[[\"dataset_source\",\"product_id\",\"image_path\"]]"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
